{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import display_img, process\n",
    "from latent_dataset import WPlusLatentsDataset\n",
    "from SAE import SparseAutoencoder\n",
    "import dnnlib\n",
    "import legacy\n",
    "from latent_dataset import save_latents\n",
    "from torch.utils.data import DataLoader\n",
    "from evaluate_saes import test_reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '/Users/spra/Desktop/Personal Projects/CV_final_project/G_w_train_latents'\n",
    "train_data = WPlusLatentsDataset(TRAIN_PATH)\n",
    "VAL_PATH = '/Users/spra/Desktop/Personal Projects/CV_final_project/G_w_val_latents'\n",
    "val_data = WPlusLatentsDataset(VAL_PATH)\n",
    "TEST_PATH = '/Users/spra/Desktop/Personal Projects/CV_final_project/G_w_test_latents'\n",
    "test_data = WPlusLatentsDataset(TEST_PATH)\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "val_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAE = torch.load('/Users/spra/Desktop/Personal Projects/CV_final_project/Image-Steering-SAEs/sae1024_10epochs.pt', weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, reconstructed_imgs = test_reconstructions(G, SAE, test_loader, h = 4, w = 4, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 128\n",
    "sae = SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn((num_images, G.z_dim), device = device)\n",
    "c = None\n",
    "with torch.no_grad():\n",
    "    w = G.mapping(noise, c)\n",
    "    # to take out the middle dimension of 14\n",
    "    w = w[:, 0, :]\n",
    "    reconstructions, sparse_representations = sae(w)\n",
    "    w = einops.repeat(w, 'b d -> b l d', l = 14)\n",
    "    images = G.synthesis(w)\n",
    "    # images should be a tensor of shape (batch_size, 3, 256, 256)\n",
    "    # sparse_representations should be a tensor of shape (batch_size, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = sparse_representations.mean(dim = 0)\n",
    "maxes, freq_indices = torch.topk(freqs, k = 20)\n",
    "print(f'Maxes: {maxes}')\n",
    "print(f'Indices: {freq_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent(sparse_representations, images, idx):\n",
    "    # top_indices and bottom_indices are tensors of shape (k, latent_dim), which is usually (16, 1024)\n",
    "    topk, top_indices = torch.topk(sparse_representations, k = 16, dim = 0, largest = True)\n",
    "    bottomk, bottom_indices = torch.topk(sparse_representations, k = 16, dim = 0, largest = False)\n",
    "    # were only taking the topk for the latent we choose using idx\n",
    "    top_chosen_indices = top_indices[:, idx]\n",
    "    top_activations = topk[:, idx]\n",
    "    bottom_chosen_indices = bottom_indices[:, idx]\n",
    "    bottom_activations = bottomk[:, idx]\n",
    "    top_images = process(images[top_chosen_indices])\n",
    "    bottom_images = process(images[bottom_chosen_indices])\n",
    "    return top_images, bottom_images, top_activations, bottom_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top, bottom, top_activations, bottom_activations = visualize_latent(sparse_representations, images, 2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
